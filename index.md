{:.no_toc}

# Authors
<ul>
    <li> <b>Shengpeng Ji</b>  zhejiang University</li>
    <li> <b>Jialong Zuo</b>  Zhejiang University</li>
    <li> <b>Minghui Fang</b>  Zhejiang University</li>
    <li> <b>Ziyue Jiang</b>  Zhejiang University</li>
    <li> <b>Feiyang Chen</b>  Huawei Cloud</li>
    <li> <b>Xinyu Duan</b>  Huawei Cloud</li>
    <li> <b>Baoxing Hua</b>  Huawei Cloud</li>
    <li> <b>Zhou Zhao</b>  Zhejiang University  Corresponding author</li>
</ul>

<br>

# Abstract
Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality text style prompt speech datasets and the absence of advanced text controllable TTS models. In light of this, 1) we propose Stylespeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of natural text style prompt descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task.

<br>

# Stylespeech
We have released a demo version containing 500 prompts on this page. If you want to get the full dataset, please contact the following email address.
<ul>
<li> <a href="mailto:shengpengji@zju.edu.cn">shengpengji@zju.edu.cn</a> </li>
<li> <a href="mailto:jialongzuo@zju.edu.cn">jialongzuo@zju.edu.cn</a> </li>
<li> <a href="mailto:minghuifang@zju.edu.cn">minghuifang@zju.edu.cn</a> </li>
</ul>
    
<br>

# Statistical Characteristics of Datasets

<br>

# Comparison of Different Speech Styles

<br>

# Comparison of Different Style Prompts

<br>


